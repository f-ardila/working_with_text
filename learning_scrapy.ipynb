{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import codecs\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://medium.com/tag/wellness/archive/']\n"
     ]
    }
   ],
   "source": [
    "tag = 'wellness'\n",
    "start_urls = ['https://medium.com/tag/'+tag+'/archive/']\n",
    "print(start_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "startDate=datetime.strptime('20200101',\"%Y%m%d\")\n",
    "endDate=datetime.strptime('20200601',\"%Y%m%d\")\n",
    "delta=endDate-startDate\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self,response):\n",
    "        response_data=response.text\n",
    "        response_split=response_data.split(\"while(1);</x>\")\n",
    "        print(response_split)\n",
    "        print(len(response_split))\n",
    "        response_data=response_split[1]\n",
    "        date_post=response.meta['reqDate']\n",
    "        date_post=date_post.replace(\"/\",\"\")\n",
    "        directory=datetime.now().strftime(\"%Y%m%d\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        writeTofile(directory+\"//\"+self.tagSlug.replace(\"-\",\"\").strip(\"'\")+\"Tag\"+date_post+\".json\",response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/tag/wellness/archive/2020/01/01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<GET https://medium.com/tag/wellness/archive/2020/01/01>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = start_urls[0]\n",
    "d=datetime.strftime(startDate,'%Y/%m/%d')\n",
    "print(url+d)\n",
    "scrapy.Request(url+d,method=\"GET\",callback=parse,meta={'reqDate':d})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTofile(fileName,text):\n",
    "    with codecs.open(fileName,'w','utf-8') as outfile:\n",
    "        outfile.write(text)\n",
    "\n",
    "class MediumPost(scrapy.Spider):\n",
    "    name='medium_scrapper'\n",
    "    handle_httpstatus_list = [401,400]\n",
    "\n",
    "    autothrottle_enabled=True\n",
    "    def start_requests(self):\n",
    "\n",
    "        start_urls = ['https://medium.com/tag/'+self.tagSlug.strip(\"'\")+'/archive/']\n",
    "        print(start_urls)\n",
    "\n",
    "        #Header and cookie information can be got from the Network Tab in Developer Tools\n",
    "        # cookie=cookie\n",
    "        # header =header\n",
    "\n",
    "        cookie = None\n",
    "        header = None\n",
    "\n",
    "        startDate=datetime.strptime(self.start_date,\"%Y%m%d\")\n",
    "        endDate=datetime.strptime(self.end_date,\"%Y%m%d\")\n",
    "        delta=endDate-startDate\n",
    "        print(delta)\n",
    "        for i in range(delta.days + 1):\n",
    "            d=datetime.strftime(startDate+timedelta(days=i),'%Y/%m/%d')\n",
    "            for url in start_urls:\n",
    "                print(url+d)\n",
    "                yield scrapy.Request(url+d,method=\"GET\",headers=header,cookies=cookie,callback=self.parse,meta={'reqDate':d})\n",
    "\n",
    "        #for url in start_urls:\n",
    "            #yield scrapy.Request(url,method='GET',headers=header,cookies=cookie,callback=self.parse)\n",
    "            #yield scrapy.Request(url,method='GET',body=json.dumps(formdata),headers=header,cookies=cookie,callback=self.parse)\n",
    "\n",
    "    def parse(self,response):\n",
    "        response_data=response.text\n",
    "        response_split=response_data.split(\"while(1);</x>\")\n",
    "        print(response_split)\n",
    "        print(len(response_split))\n",
    "        response_data=response_split[1]\n",
    "        date_post=response.meta['reqDate']\n",
    "        date_post=date_post.replace(\"/\",\"\")\n",
    "        directory=datetime.now().strftime(\"%Y%m%d\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        writeTofile(directory+\"//\"+self.tagSlug.replace(\"-\",\"\").strip(\"'\")+\"Tag\"+date_post+\".json\",response_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
